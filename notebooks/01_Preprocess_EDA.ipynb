{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a78106",
   "metadata": {},
   "source": [
    "# Data loading, Preprocess and EDA\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15def9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m../data/raw/DataCoSupplyChainDataset.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m df.head()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Logistiq Prediction\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9219f9e",
   "metadata": {},
   "source": [
    "## Load data with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e342d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"../data/raw/data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53218b65",
   "metadata": {},
   "source": [
    "## Analyse Exploratoire des Donn√©es (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f58a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.show(5)\n",
    "print(f\"Number of rows: {df.count()}\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d1db98",
   "metadata": {},
   "source": [
    "* Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120576eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df.select(\n",
    "    # Target\n",
    "    \"Late_delivery_risk\",\n",
    "    \n",
    "    # Order and shipment details\n",
    "    \"Order Id\",\n",
    "    \"order date (DateOrders)\",\n",
    "    \"shipping date (DateOrders)\",\n",
    "    \"Order Status\",\n",
    "    \"Delivery Status\",\n",
    "    \"Days for shipping (real)\",\n",
    "    \"Days for shipment (scheduled)\",\n",
    "    \"Shipping Mode\",\n",
    "    \n",
    "    # Sales & financials\n",
    "    \"Sales\",\n",
    "    \"Sales per customer\",\n",
    "    \"Benefit per order\",\n",
    "    \"Order Profit Per Order\",\n",
    "    \"Order Item Quantity\",\n",
    "    \"Order Item Discount\",\n",
    "    \"Order Item Discount Rate\",\n",
    "    \"Order Item Product Price\",\n",
    "    \"Order Item Profit Ratio\",\n",
    "    \"Order Item Total\",\n",
    "    \n",
    "    # Product details\n",
    "    \"Product Card Id\",\n",
    "    \"Product Name\",\n",
    "    \"Product Price\",\n",
    "    \"Product Category Id\",\n",
    "    \"Product Card Id\",\n",
    "    \"Product Status\",\n",
    "    \"Category Name\",\n",
    "    \"Department Name\",\n",
    "    \n",
    "    # Customer info (excluding private fields)\n",
    "    \"Customer Id\",\n",
    "    \"Customer Fname\",\n",
    "    \"Customer Lname\",\n",
    "    \"Customer Segment\",\n",
    "    \"Customer City\",\n",
    "    \"Customer State\",\n",
    "    \"Customer Country\",\n",
    "    \"Customer Zipcode\",\n",
    "    \n",
    "    # Order region info\n",
    "    \"Order City\",\n",
    "    \"Order State\",\n",
    "    \"Order Country\",\n",
    "    \"Order Region\",\n",
    "    \"Order Zipcode\",\n",
    "    \n",
    "    # Geolocation\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \n",
    "    # Market context\n",
    "    \"Market\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef533c",
   "metadata": {},
   "source": [
    "* Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51655929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "df_selected.select([count(when(col(c).isNull(), c)).alias(c) for c in df_selected.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767cd68",
   "metadata": {},
   "source": [
    "* Counts for categorical features,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d0ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expected categorical columns (can include columns not present in df_selected)\n",
    "categorical_cols = [\n",
    "    \"Type\",\n",
    "    \"Delivery Status\",\n",
    "    \"Order Status\",\n",
    "    \"Shipping Mode\",\n",
    "    \"Customer Segment\",\n",
    "    \"Market\",\n",
    "    \"Department Name\",\n",
    "    \"Category Name\",\n",
    "    \"Order Region\",\n",
    "    \"Order Country\",\n",
    "    \"Order City\"\n",
    "]\n",
    "\n",
    "# Filter to columns that actually exist in df_selected to avoid AnalysisException\n",
    "present_categorical_cols = [c for c in categorical_cols if c in df_selected.columns]\n",
    "missing = [c for c in categorical_cols if c not in present_categorical_cols]\n",
    "if missing:\n",
    "    print(f\"Warning: these categorical columns are not present in df_selected: {', '.join(missing)}\")\n",
    "\n",
    "# Perform group counts only on present columns\n",
    "for col_name in present_categorical_cols:\n",
    "    df_selected.groupBy(col_name).count().orderBy(\"count\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dea06e",
   "metadata": {},
   "source": [
    "* Stats for numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    \"Days for shipping (real)\",\n",
    "    \"Days for shipment (scheduled)\",\n",
    "    \"Sales per customer\",\n",
    "    \"Order Item Quantity\",\n",
    "    \"Order Item Discount\",\n",
    "    \"Order Item Discount Rate\",\n",
    "    \"Order Item Product Price\",\n",
    "    \"Order Item Total\",\n",
    "    \"Order Item Profit Ratio\",\n",
    "    \"Sales\",\n",
    "    \"Benefit per order\",\n",
    "    \"Order Profit Per Order\",\n",
    "    \"Product Price\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"Customer Zipcode\",\n",
    "    \"Order Zipcode\"\n",
    "]\n",
    "\n",
    "df_selected.describe(numeric_cols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46624d01",
   "metadata": {},
   "source": [
    "* Histogram for numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d4a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_selected.select(numeric_cols).toPandas().hist(bins=20, figsize=(15,10))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe29a6f",
   "metadata": {},
   "source": [
    "* Explore correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import DoubleType\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "df_vector = assembler.transform(df_selected.select([col(c).cast(DoubleType()) for c in numeric_cols]))\n",
    "\n",
    "corr_matrix = Correlation.corr(df_vector, \"features\").head()[0].toArray()\n",
    "\n",
    "corr_df = pd.DataFrame(corr_matrix, index=numeric_cols, columns=numeric_cols)\n",
    "\n",
    "plt.figure(figsize=(8, 6)) \n",
    "sns.heatmap(corr_df, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
